# -*- coding: utf-8 -*-
"""Customer_Churn_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zebC_qiBt2-IJTkoQau0A18VGVNNvc7I
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier

import matplotlib.pyplot as plt

data= pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')

data.head()

data.shape

data.describe()

"""SeniorCitizen is actually a categorical hence the 25%-50%-75% distribution is not propoer

75% customers have tenure less than 55 months

Average Monthly charges are USD 64.76 whereas 25% customers pay more than USD 89.85 per month
"""

data.info()

data['Churn'].value_counts().plot(kind ='barh', figsize=(8,6))
plt.xlabel('count')
plt.ylabel('Target variable')
plt.title('Churn analyse')
plt.show()

100*data['Churn'].value_counts()/len(data['Churn'])

data['Churn'].value_counts()

"""Data is highly imbalanced, ratio = 73:27.\
So we analyse the data with other features while taking the target values separately to get some insights.
"""

data.isnull().sum()

missing = pd.DataFrame((data.isnull().sum())*100/data.shape[0]).reset_index()
plt.figure(figsize=(16,5))
plt.xticks(rotation = 90, fontsize = 7)
sns.pointplot('index',0, data = missing)
plt.ylabel('Percentage')
plt.title('percentage of missing value')
plt.show()

"""
Missing Data - Initial Intuition
Here, we don't have any missing data.
General Thumb Rules:

For features with less missing values- can use regression to predict the missing values or fill with the mean of the values present, depending on the feature.
For features with very high number of missing values- it is better to drop those columns as they give very less insight on analysis.

As there's no thumb rule on what criteria do we delete the columns with high number of missing values, but generally you can delete the columns, if you have more than 30-40% of missing values. But again there's a catch here, for example, Is_Car & Car_Type, People having no cars, will obviously have Car_Type as NaN (null), but that doesn't make this column useless, so decisions has to be taken wisely."""



data['TotalCharges'].isnull().sum()

data1 = data.copy()

data1.TotalCharges =pd.to_numeric(data1.TotalCharges,errors = 'coerce')
data1.isnull().sum()

data1.loc[data1['TotalCharges'].isnull() == True]

"""Since the % of these records compared to total dataset is very low ie 0.15%, it is safe to ignore them from further processing."""

data1.dropna(how= 'any', inplace = True)

"""5. Divide customers into bins based on tenure e.g. for tenure < 12 months: assign a tenure group if 1-12, for tenure between 1 to 2 Yrs, tenure group of 13-24; so on."""

print(data1['tenure'].max())

print(data1['tenure'].min())

#grop the tenture in bins of 12 months
labels = ['{0} -{1} ' . format(i, i + 11) for  i in range(1,72,12)]
data1['tenure_group'] = pd.cut(data1.tenure,range(1,80,12), right = False, labels = labels)

data1['tenure_group'].value_counts()

data1.drop(columns=['customerID','tenure'], axis= 1, inplace= True)

data1.head()

"""Data Exploration
1. Plot distibution of individual predictors by churn

Univariate Analysis
"""

for i, predictor in enumerate(data1.drop(columns = ['Churn','TotalCharges','MonthlyCharges'])) :
  plt.figure(i)
sns.countplot(data=data1, x = predictor, hue = 'Churn')

"""2. Convert the target variable 'Churn' in a binary numeric variable i.e. Yes=1 ; No = 0"""

data1['Churn'] = np.where(data1.Churn == 'Yes',1,0)

data1['Churn'].unique()

data1['Churn'].value_counts()

data1['Churn'].value_counts()*100/len(data['Churn'])

"""3. Convert all the categorical variables into dummy variables"""

data1_dummies = pd.get_dummies(data1)

data1_dummies.head()

data1_dummies.shape

"""Relationship between Monthly Charges and Total Charges"""

'''sns.lmplot(data=telco_data_dummies, x='MonthlyCharges', y='TotalCharges', fit_reg=False)'''
sns.lmplot(data=data1_dummies, x = 'MonthlyCharges', y = 'TotalCharges', fit_reg= False)

"""Churn by Monthly Charges and Total Charges

Total Charges increase as Monthly Charges increase - as expected.
"""

month = sns.kdeplot(data1_dummies.MonthlyCharges[(data1_dummies['Churn'] == 0)],color='Red', shade=True)
month = sns.kdeplot(data1_dummies.MonthlyCharges[(data1_dummies['Churn'] == 1)],color='Blue', shade=True)
month.legend(['No Churn','Churn'], loc='upper right')
month.set_ylabel('Density')
month.set_xlabel('Monthly Charges')
month.set_title('Monthly charges by churn')

"""Insight: Churn is high when Monthly Charges ar high"""

tot = sns.kdeplot(data1_dummies.TotalCharges[(data1_dummies['Churn'] == 0)], color = 'Red', shade=True)
tot = sns.kdeplot(data1_dummies.TotalCharges[(data1_dummies['Churn'] == 1)],color= 'Blue', shade= True)
tot.legend(['NoChurn','Churn'],loc='upper righ')
tot.set_xlabel('TotalCharges')
tot.set_ylabel('Density')
tot.set_title('TotalCharges by Churn')

"""Surprising insight as higher Churn at lower Total Charges

However if we combine the insights of 3 parameters i.e. Tenure, Monthly Charges & Total Charges then the picture is bit clear :- Higher Monthly Charge at lower tenure results into lower Total Charge. Hence, all these 3 factors viz Higher Monthly Charge, Lower tenure and Lower Total Charge are linkd to High Churn.

Build a corelation of all predictors with 'Churn'
"""

plt.figure(figsize=(20,8))
data1_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')

"""
Derived Insight:

HIGH Churn seen in case of Month to month contracts, No online security, No Tech support, First year of subscription and Fibre Optics Internet

LOW Churn is seens in case of Long term contracts, Subscriptions without internet service and The customers engaged for 5+ years

Factors like Gender, Availability of PhoneService and # of multiple lines have alomost NO impact on Churn

This is also evident from the Heatmap below"""

plt.plot(figsize=(15,16))
sns.heatmap(data1_dummies.corr(), cmap = 'Paired')

data1_dummies.corr().sum()

"""
Bivariate Analysis"""

new_df1_target0=data1.loc[data1["Churn"]==0]
new_df1_target1=data1.loc[data1["Churn"]==1]

def uniplot(df,col,title,hue =None):
    
    sns.set_style('whitegrid')
    sns.set_context('talk')
    plt.rcParams["axes.labelsize"] = 20
    plt.rcParams['axes.titlesize'] = 22
    plt.rcParams['axes.titlepad'] = 30
    
    
    temp = pd.Series(data = hue)
    fig, ax = plt.subplots()
    fig.set_size_inches(10, 8)
    plt.xticks(rotation=45)
    plt.title(title)
    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='bright') 
        
    plt.show()

uniplot(new_df1_target1,col='Partner',title='Distribution of Gender for Churned Customers',hue='gender')

uniplot(new_df1_target0,col='Partner',title='Distribution of Gender for Non Churned Customers',hue='gender')

uniplot(new_df1_target1,col='PaymentMethod',title='Distribution of PaymentMethod for Churned Customers',hue='gender')

uniplot(new_df1_target1,col='Contract',title='Distribution of Contract for Churned Customers',hue='gender')

uniplot(new_df1_target1,col='TechSupport',title='Distribution of TechSupport for Churned Customers',hue='gender')

uniplot(new_df1_target1,col='SeniorCitizen',title='Distribution of SeniorCitizen for Churned Customers',hue='gender')

"""
CONCLUSION

These are some of the quick insights from this exercise:

1.Electronic check medium are the highest churners

2.Contract Type - Monthly customers are more likely to churn because of no contract terms, as they are free to go customers.

3.No Online security, No Tech Support category are high churners

4.Non senior Citizens are high churners
"""

data1_dummies.to_csv('tel_churn.csv')

"""Model_Building"""

import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from imblearn.combine import SMOTEENN

df = pd.read_csv('/content/tel_churn.csv')

df.head()

df = df.drop('Unnamed: 0', axis=1)

x = df.drop('Churn', axis=1)

y = df['Churn']

x

y



"""Train_Test_Split"""

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 2)



"""Decision _tree_classifier"""

model = DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=8, random_state=100)

model.fit(x_train, y_train)

y_predict= model.predict(x_test)
y_predict

model.score(x_test, y_test)

print(classification_report(y_test, y_predict))

print(confusion_matrix(y_test, y_predict))

"""As you can see that the accuracy is quite low, and as it's an imbalanced dataset, we shouldn't consider Accuracy as our metrics to measure the model, as Accuracy is cursed in imbalanced datasets.

Hence, we need to check recall, precision & f1 score for the minority class, and it's quite evident that the precision, recall & f1 score is too low for Class 1, i.e. churned customers.

Hence, moving ahead to call SMOTEENN (UpSampling + ENN)
"""

sm = SMOTEENN()

X_resampled, Y_resampled = sm.fit_sample(x,y)

xr_train, xr_test, yr_train, yr_test = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=2)

model.fit(xr_train, yr_train)

y_predict = model.predict(xr_test)

y_predict

model.score(xr_test,yr_test)

print(classification_report(yr_test,y_predict))

print(confusion_matrix(yr_test, y_predict))

"""Now we can see quite better results, i.e. Accuracy: 92 %, and a very good recall, precision & f1 score for minority class.

Let's try with some other classifier.

Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100,max_depth=6, min_samples_leaf=8,criterion='gini')

model.fit(x_train, y_train)

y_predict= model.predict(x_test)

y_predict

model.score(x_test, y_test)

print(classification_report(y_test, y_predict))

print(confusion_matrix(y_test, y_predict))

sm = SMOTEENN()

X_resampled, Y_resampled = sm.fit_sample(x,y)

xr_train, xr_test, yr_train,yr_test = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=2)

model = RandomForestClassifier(n_estimators=100, criterion='gini',max_depth=8,min_samples_leaf=6, random_state=100)

model.fit(xr_train, yr_train)

y_predict = model.predict(xr_test)

y_predict

print(classification_report(yr_test,y_predict))

print(confusion_matrix(yr_test,y_predict))

df.shape

"""since the shape of the data is much more high , so we do principle component analyse to reduce the dimensionality of the data"""

from sklearn.decomposition import PCA

pca = PCA(0.9)

xr_train_pca = pca.fit_transform(xr_train)

xr_test_pca = pca.fit_transform(xr_test)

model.fit(xr_train_pca,yr_train)

y_predict = model.predict(xr_test_pca)

y_predict

model.score(xr_test_pca,yr_test)

print(classification_report(yr_test,y_predict))

print(confusion_matrix(yr_test, y_predict))

"""With PCA, we couldn't see any better results, hence let's finalise the model which was created by RF Classifier, and save the model so that we can use it in a later stage :

Note: we use the SMOTEENN sample data to perform pca, and we come to the conculsion that there is no better result in accuracy,
"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

model.fit(xr_train_pca,yr_train)

y_predict = model.predict(xr_test_pca)

model.score(xr_test_pca,yr_test)

model.fit(xr_train,yr_train)

y_predict = model.predict(xr_test)

model.score(xr_test,yr_test)

print(classification_report(yr_test,y_predict))

print(confusion_matrix(yr_test,y_predict))

"""Built a Basic Model for Prediction"""

from sklearn.model_selection import GridSearchCV

params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=model, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(xr_train,yr_train)

y_predict = grid_search.predict(xr_test)

grid_search.score(xr_test,yr_test)

print(classification_report(yr_test,y_predict))

print(confusion_matrix(yr_test,y_predict))

"""we get good results by Tuning using Grid_searchCV """

input_data = [0,	29.85,	29.85,		1,	0,	0,	1,	1,	0,	1,	0,	0,	1,	0,	1,	0,	0,	1,	0,	0,	0,	0,	1,	1,	0,	0,	1,	0,	0,	1,	0,	0,	1,	0,	0,	1,	0,	0,	0,	1,	0,	0,	1,	0,	1,	0,	0,	0,	0,	0]
result = np.asarray(input_data)
result_reshape = result.reshape(1,-1)
from sklearn.preprocessing import StandardScaler
std = StandardScaler()
result_std = std.fit_transform(result_reshape)
print(result_std)
prediction = grid_search.predict(result_std)
print(prediction)

if(prediction[0]== 1):
  print('the customer is churn')
else:
  print('the customer is not churn')

"""Pickle Saving"""

import pickle

filename = 'model.sav'

pickle.dump(grid_search, open(filename,'wb'))

load_model = pickle.load(open(filename,'rb'))

model_score_r = load_model.score(xr_test, yr_test)

print(model_score_r)

"""Our final model i.e. Grid_searchCV  with SMOTEENN, is now ready and dumped in model.sav, which we will use and prepare API's so that we can access our model from UI.

We need to make the initial givn data to input_data for our model by removing label column and unwanted column
"""

df2 = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')

df2.head()

df2 = df2.drop(columns=['customerID','Churn'],axis=1)

df2.head()

df2.to_csv('input_tel.csv')

"""Now we have a input_data , we can use this input data in our app.py file to create the API"""

